#!/bin/bash
#Submit this script with: sbatch thefilename

#SBATCH --time=00:08:00   # walltime
#SBATCH --mem=4000M               # memory (per node)
#SBATCH --gres=gpu:1
#SBATCH --nodes=1

#SBATCH --ntasks=1   # number of processor cores (i.e. tasks)
##SBATCH --gpu-per-node=1
#SBATCH --sockets-per-node=1
#SBATCH --cores-per-socket=2
#SBATCH --ntasks-per-node=1 #for distributed memory mpi
#SBATCH --ntasks-per-socket=1
##SBATCH --cpus-per-gpu=2
#SBATCH --cpus-per-task=2  #for shared memory openmp

#SBATCH -J "20190708"   # job name
#SBATCH --mail-user=hllin@caltech.edu   # email address
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL

#SBATCH --qos=debug
# RETURN ENV
echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR
echo "SLURM_GPUS_PER_NODE"=$SLURM_GPUS_PER_NODE
echo "working directory = "$SLURM_SUBMIT_DIR

#ulimit -s 10240
export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK}
#export OMP_PROC_BIND=true
export OMP_PROC_BIND=spread
export OMP_PLACES=threads
#export PSM2_GPUDIRECT=1
#echo $OMPI_MCA_pml
#OMPI_MCA_pml=ucx
#export OMPI_MCA_pml
#LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE

nvidia-smi

lscpu
env
echo "CUDA_check"
echo $CUDA_ENABLE_CRC_CHECK
ompi_info --parsable --all | grep mpi_built_with_cuda_support:value
ompi_info --all | grep btl_openib_have_cuda_gdr
ompi_info --all | grep btl_openib_have_driver_gdr
echo $PSM2_CUDA
service nv_peer_mem status
lsmod | grep nv_peer_mem
lsmod | grep gdrdrv

export OMPI_MCA_btl=^openib
export OMPI_MCA_btl_openib_want_cuda_gdr=1





#srun
SRUN_SCRIPT="srun --mpi=pmi2"

#ompi

#ompi bind
#socket
#core
#thread
MPIRUN_SCRIPT_basic="mpirun"
MPIRUN_SCRIPT_avoidOFwarning="mpirun --mca btl ^openib"

#mvapich
#socket
#core
#thread
MPIRUN_SCRIPT_gpudirectRDMA="mpirun --mca btl_openib_want_cuda_gdr 1"
MPIRUN_SCRIPT_gpudirectRDMA_bind_c="mpirun --mca btl_openib_want_cuda_gdr 1 --bind-to core --map-by core"
MPIRUN_SCRIPT_avoidOFwarning_bind_c="mpirun --bind-to core --map-by core --mca btl ^openib"
MPIRUN_SCRIPT_avoidOFwarning_bind_s="mpirun --bind-to socket --map-by socket --mca btl ^openib"
MPIRUN_SCRIPT_ucx="mpirun -np 2 -mca pml ucx -mca btl ^uct -x"

LMP_CMD_kno="-k off -in"
LMP_CMD_kno_sfkk="-k off -pk kokkos -sf kk -in"
LMP_CMD_kno_sfkk_neha="-k off -pk kokkos neigh half -sf kk -in"

LMP_CMD_kyes="-k on g 1 -pk kokkos neigh half gpu/direct off -sf kk -in"
LMP_CMD_kyes_gpuomp="-k on g 1 t 2 -pk kokkos neigh half gpu/direct off -sf kk -in"
LMP_CMD_kyes_directon="-k on g 1 -pk kokkos neigh half -sf kk -in"
LMP_CMD_kyes_directon_gpuomp="-k on g 1 t 2 -pk kokkos neigh half -sf kk -in"

LMP_INSCRIPT_sm="/home/hllin/python/simulation/lammps_input_script/in.lmpscript_simple_20190114_v11"
LMP_INSCRIPT_kkbench_kokkos="/home/hllin/python/simulation/lammps_input_script/kokkos_bench/in.chute_kokkos"
LMP_INSCRIPT_kkbench_kokkos_print1="/home/hllin/python/simulation/lammps_input_script/kokkos_bench/in.chute_kokkos_print1atom"
LMP_INSCRIPT_ch="in.chute_print1atom"
LMP_INSCRIPT_lj="in.lj_print1atom"
LMP_INSCRIPT_tmp="/home/hllin/python/simulation/lammps_input_script/in.lmpscript_20190114_v11"

LMP_INSCRIPT=${LMP_INSCRIPT_tmp}
MPIRUN_SCRIPT=${MPIRUN_SCRIPT_avoidOFwarning_bind}



module purge
#module load cuda/10.0 ompi/4.0.1_cuda10.0_noucx lmp/190718master_ompi401_cuda10_noucx
module load cuda/10.0 ompi/4.0.1_cuda10.0_yesucx lmp/190719master_ompi401_cuda10_yesucx
module list
${MPIRUN_SCRIPT_avoidOFwarning} lmp ${LMP_CMD_kno} ${LMP_INSCRIPT}  # 14coreMPI best
${MPIRUN_SCRIPT_avoidOFwarning} lmp ${LMP_CMD_kyes} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT_avoidOFwarning} lmp ${LMP_CMD_kyes_gpuomp} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT_avoidOFwarning} lmp ${LMP_CMD_kyes_directon_gpuomp} ${LMP_INSCRIPT}


#${MPIRUN_SCRIPT_gpudirectRDMA} lmp ${LMP_CMD_kyes} ${LMP_INSCRIPT} #1 core best
#${MPIRUN_SCRIPT_gpudirectRDMA_bind_c} lmp ${LMP_CMD_kyes} ${LMP_INSCRIPT} #1core best 

#${SRUN_SCRIPT} lmp ${LMP_CMD_kyes} ${LMP_INSCRIPT}
#${MPIRUN_SCRIPT_avoidOFwarning_bind_c} lmp ${LMP_CMD_kno} ${LMP_INSCRIPT} # 14coreMPI best
#${MPIRUN_SCRIPT_avoidOFwarning_bind_c} lmp ${LMP_CMD_kyes} ${LMP_INSCRIPT}

#${MPIRUN_SCRIPT_avoidOFwarning_bind_s} lmp ${LMP_CMD_kno} ${LMP_INSCRIPT} #very bad
#${MPIRUN_SCRIPT_avoidOFwarning_bind_s} lmp ${LMP_CMD_kyes} ${LMP_INSCRIPT} #very bad

#${MPIRUN_SCRIPT_avoidOFwarning} lmp ${LMP_CMD_kno} ${LMP_INSCRIPT}  # 14coreMPI best
#${MPIRUN_SCRIPT_avoidOFwarning} lmp ${LMP_CMD_kyes} ${LMP_INSCRIPT}

#comment
:'
module purge
module load cuda/8.0 openmpi/3.1.4
#module load cuda/10.0 ompi/3.1.4_cuda10.0 #openmpi/3.0.0  mpich/3.2.1
#module load ucx/1.6_cuda10.0_nonuma cuda/10.0 ompi/3.1.4_ucx1.6_cuda10.0_nonuma
module list
${MPIRUN_SCRIPT} ${LMP_EXE_clusteropenmpi} ${LMP_CMD_kno} ${LMP_INSCRIPT} 
${MPIRUN_SCRIPT} ${LMP_EXE_clusteropenmpi} ${LMP_CMD_kno} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_clusteropenmpi} ${LMP_CMD_kyes} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_clusteropenmpi} ${LMP_CMD_kyes_directon} ${LMP_INSCRIPT}

module purge
module load cuda/10.0 ompi/3.1.4_cuda10.0 #openmpi/3.0.0  mpich/3.2.1
module list
${MPIRUN_SCRIPT} ${LMP_EXE_cuda10} ${LMP_CMD_kno} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_cuda10} ${LMP_CMD_kno} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_cuda10} ${LMP_CMD_kyes} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_cuda10} ${LMP_CMD_kyes_directon} ${LMP_INSCRIPT}


module purge
module load ucx/1.6_cuda10.0_nonuma cuda/10.0 ompi/3.1.4_ucx1.6_cuda10.0_nonuma
module list
${MPIRUN_SCRIPT} ${LMP_EXE_ucx16_cuda100_nonuma} ${LMP_CMD_kno} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_ucx16_cuda100_nonuma} ${LMP_CMD_kno} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_ucx16_cuda100_nonuma} ${LMP_CMD_kyes} ${LMP_INSCRIPT}
${MPIRUN_SCRIPT} ${LMP_EXE_ucx16_cuda100_nonuma} ${LMP_CMD_kyes_directon} ${LMP_INSCRIPT}

#${SRUN_SCRIPT} ${LMP_EXE_cuda10} ${LMP_CMD_kyes_directon} ${LMP_INSCRIPT_kkbench_kokkos}
#${SRUN_SCRIPT} ${LMP_EXE_ucx16_cuda100_nonuma} ${LMP_CMD_kyes} ${LMP_INSCRIPT_kkbench_kokkos}


#mpirun ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda/lmp -pk kokkos neigh half gpu/direct off -k on g 1 -sf kk -in ~/python/simulation/lammps_input_script/in.lmpscript_simple_20190114_v11 #${SLURM_CPUS_PER_TASK}
#mpirun ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda/lmp -pk kokkos neigh half -k on g 1 -sf kk -in ~/python/simulation/lammps_input_script/in.lmpscript_simple_20190114_v11 #${SLURM_CPUS_PER_TASK}
#srun --mpi=pmi2 ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda/lmp -pk kokkos neigh half gpu/direct off -k on g 1 -sf kk -in ~/python/simulation/lammps_input_script/in.lmpscript_simple_20190114_v11 #${SLURM_CPUS_PER_TASK}
#srun --mpi=pmi2 ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda/lmp -pk kokkos neigh half -k on g 1 -sf kk -in ~/python/simulation/lammps_input_script/in.lmpscript_simple_20190114_v11 #${SLURM_CPUS_PER_TASK}

#srun --mpi=pmi2 ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda/lmp -pk kokkos neigh half -k on g 1 -sf kk -in in.chute #${SLURM_CPUS_PER_TASK}
#srun --mpi=pmi2 ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda_debug/lmp -k on g 2 -pk kokkos neigh half -sf kk -in in.lj #${SLURM_CPUS_PER_TASK}
#srun --mpi=pmi2 ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda_debug/lmp -k off -in in.lj #${SLURM_CPUS_PER_TASK}
#mpirun ~/lmp_exe/20190712_mymodule_unstable_openmpi314_granular_kkcuda/lmp -k on g 1 -pk kokkos neigh half -sf kk -in in.lj #${SLURM_CPUS_PER_TASK}
'
echo "All Done!"
